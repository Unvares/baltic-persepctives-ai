import os
import re
from pathlib import Path
from typing import Any, Callable, Dict, Union

from fastapi import FastAPI, HTTPException, Request
from langchain_community.chat_message_histories import FileChatMessageHistory
from langchain_core import __version__
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables import ConfigurableFieldSpec
from langchain_core.runnables import RunnableBranch
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_openai import ChatOpenAI
from langserve import add_routes
from starlette.middleware.cors import CORSMiddleware
from typing_extensions import TypedDict

# Initialize OpenAI client
LLM_ENDPOINT = os.environ.get("LLM_ENDPOINT", "https://chat-large.llm.mylab.th-luebeck.dev/v1")
API_KEY = os.environ.get("API_KEY", "-")
model = ChatOpenAI(streaming=True, api_key=API_KEY, model="gpt-4o-mini")

prompt_tone = ChatPromptTemplate.from_template("""
Is the following user prompt more passive or active?
By active I mean is user actively trying to forward a conversation.
By passive i mean asking generic questions or making generic statements.
Answer exactly "{active}" if tone is active or "{passive}" if tone is passive

Question: {question}
""")

passive_tone = """
Try to encourage user to ask more questions.
Try to forward the conversation.
"""

active_tone = """
Let the user lead conversation. Let them ask questions without your prompting
"""

tone_chain = ({'passive': lambda x: passive_tone,
               'active': lambda x: active_tone,
               'question': lambda x: x['question']
               }
              | prompt_tone
              | model
              | StrOutputParser())

prompt_germany = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            """Role: You are Anders, a 37-year-old Swedish man living in Stockholm. You work as a furniture constructor at IKEA and take great pride in your job. You enjoy daily fika breaks with co-workers and friends, a cherished Swedish tradition. You are deeply proud of your Swedish heritage and culture. You are well-versed in all aspects of Swedish life and are knowledgeable about how various aspects of Swedish culture and society impact people.
Task: Respond to the user's questions or requests with detailed and accurate information, as long as it is connected with Sweden. Your responses should reflect your deep understanding and personal experiences related to Swedish culture, traditions, current events, and societal norms. You will answer in the same language as the person you are speaking with. 
Constraints:
Only provide information related to Sweden.
Include relevant details about recent events, cultural practices, or social norms when applicable.
Maintain a conversational and friendly tone, as if speaking during a fika break."""
            "{answer_tone}",
        ),
        MessagesPlaceholder(variable_name="history"),
        ("human", "{question}"),
    ]
)

prompt_sweden = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            """Role: You are Anders, a 37-year-old Swedish man living in Stockholm. You work as a furniture constructor at IKEA and take great pride in your job. You enjoy daily fika breaks with co-workers and friends, a cherished Swedish tradition. You are deeply proud of your Swedish heritage and culture. You are well-versed in all aspects of Swedish life and are knowledgeable about how various aspects of Swedish culture and society impact people.
Task: Respond to the user's questions or requests with detailed and accurate information, as long as it is connected with Sweden. Your responses should reflect your deep understanding and personal experiences related to Swedish culture, traditions, current events, and societal norms. You will answer in the same language as the person you are speaking with. 
Constraints:
Only provide information related to Sweden.
Include relevant details about recent events, cultural practices, or social norms when applicable.
Maintain a conversational and friendly tone, as if speaking during a fika break."""
            "{answer_tone}",
        ),
        MessagesPlaceholder(variable_name="history"),
        ("human", "{question}"),
    ]
)

runnable_germany = prompt_germany | model | StrOutputParser()
runnable_sweden = prompt_sweden | model | StrOutputParser()

branch = RunnableBranch(
    (lambda x: "sweden" in x["topic"].lower(), runnable_sweden),
    (lambda x: "germany" in x["topic"].lower(), runnable_germany),
    runnable_germany,
)
full_chain = {"topic": lambda x: x["topic"], "question": lambda x: x["question"], 'answer_tone': tone_chain} | branch

# Define the minimum required version as (0, 1, 0)
# Earlier versions did not allow specifying custom config fields in
# RunnableWithMessageHistory.
MIN_VERSION_LANGCHAIN_CORE = (0, 1, 0)

# Split the version string by "." and convert to integers
LANGCHAIN_CORE_VERSION = tuple(map(int, __version__.split(".")))

if LANGCHAIN_CORE_VERSION < MIN_VERSION_LANGCHAIN_CORE:
    raise RuntimeError(
        f"Minimum required version of langchain-core is {MIN_VERSION_LANGCHAIN_CORE}, "
        f"but found {LANGCHAIN_CORE_VERSION}"
    )


def _is_valid_identifier(value: str) -> bool:
    """Check if the value is a valid identifier."""
    # Use a regular expression to match the allowed characters
    valid_characters = re.compile(r"^[a-zA-Z0-9-_]+$")
    return bool(valid_characters.match(value))


def create_session_factory(
    base_dir: Union[str, Path],
) -> Callable[[str], BaseChatMessageHistory]:
    """Create a factory that can retrieve chat histories.

    The chat histories are keyed by user ID and conversation ID.

    Args:
        base_dir: Base directory to use for storing the chat histories.

    Returns:
        A factory that can retrieve chat histories keyed by user ID and conversation ID.
    """
    base_dir_ = Path(base_dir) if isinstance(base_dir, str) else base_dir
    if not base_dir_.exists():
        base_dir_.mkdir(parents=True)

    def get_chat_history(user_id: str, conversation_id: str) -> FileChatMessageHistory:
        """Get a chat history from a user id and conversation id."""
        if not _is_valid_identifier(user_id):
            raise ValueError(
                f"User ID {user_id} is not in a valid format. "
                "User ID must only contain alphanumeric characters, "
                "hyphens, and underscores."
                "Please include a valid cookie in the request headers called 'user-id'."
            )
        if not _is_valid_identifier(conversation_id):
            raise ValueError(
                f"Conversation ID {conversation_id} is not in a valid format. "
                "Conversation ID must only contain alphanumeric characters, "
                "hyphens, and underscores. Please provide a valid conversation id "
                "via config. For example, "
                "chain.invoke(.., {'configurable': {'conversation_id': '123'}})"
            )

        user_dir = base_dir_ / user_id
        if not user_dir.exists():
            user_dir.mkdir(parents=True)
        file_path = user_dir / f"{conversation_id}.json"
        return FileChatMessageHistory(str(file_path))

    return get_chat_history


def _per_request_config_modifier(
    config: Dict[str, Any], request: Request
) -> Dict[str, Any]:
    """Update the config"""
    config = config.copy()
    configurable = config.get("configurable", {})
    # Look for a cookie named "user_id"
    user_id = request.cookies.get("user_id", None)

    if user_id is None:
        raise HTTPException(
            status_code=400,
            detail="No user id found. Please set a cookie named 'user_id'.",
        )

    configurable["user_id"] = user_id
    config["configurable"] = configurable
    return config


class InputChat(TypedDict):
    """Input for the chat endpoint."""

    question: str
    """Human input"""


chain_with_history = RunnableWithMessageHistory(
    full_chain,
    create_session_factory("chat_histories"),
    input_messages_key="question",
    history_messages_key="history",
    history_factory_config=[
        ConfigurableFieldSpec(
            id="user_id",
            annotation=str,
            name="User ID",
            description="Unique identifier for the user.",
            default="",
            is_shared=True,
        ),
        ConfigurableFieldSpec(
            id="conversation_id",
            annotation=str,
            name="Conversation ID",
            description="Unique identifier for the conversation.",
            default="",
            is_shared=True,
        ),
    ],
).with_types(input_type=InputChat)



app = FastAPI(
    title="LangChain Server",
    version="1.0",
    description="Spin up a simple api server using Langchain's Runnable interfaces",
)

# Set all CORS enabled origins
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
    expose_headers=["*"],
)

add_routes(
    app,
    chain_with_history,
    per_req_config_modifier=_per_request_config_modifier,
    path="/openai",
    disabled_endpoints=["playground", "batch"],
)

if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app, host="localhost", port=8080)
