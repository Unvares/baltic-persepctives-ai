{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87197cd0-f53c-4d05-8259-491a7bf480f5",
   "metadata": {},
   "source": [
    "# Hier ist Platz für Notebooks\n",
    "\n",
    "Mittels der GIT-Integration könnt ihr Änderungen direkt in unser Gitlab System pushen und so kollaborativ arbeiten.\n",
    "\n",
    "Starte, und lass dir einen Witz erzählen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5f6c35-78d1-428e-bf84-4bf6f4eb92ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFaceTextGenInference\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "LLM = \"https://em-german-13b.llm.mylab.th-luebeck.dev\"\n",
    "\n",
    "from langchain.llms import HuggingFaceTextGenInference\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "# Erstellen Sie das LLM-Objekt\n",
    "llm = HuggingFaceTextGenInference(\n",
    "    callbacks=[StreamingStdOutCallbackHandler()],  # Callbacks, wir verwenden den fürs Streaming: Raus nehmen wenn nicht streaming genutzt werden soll\n",
    "    max_new_tokens=1024,  # Die maximale Anzahl an Tokens, die generiert werden sollen\n",
    "    temperature=0.25,  # Die \"Temperatur\" beim Generieren, gibt an wie\n",
    "    inference_server_url=LLM,  # URL des Inferenzservers\n",
    "    repetition_penalty=1.4, # Strafe für Wiederholungen\n",
    "    timeout=10,  # Timeout in Sekunden für die Verbindung zum Inferenzserver\n",
    "    streaming=True,  # Ob die Antwort gestreamt werden soll\n",
    ")\n",
    "\n",
    "witz = llm(\"USER:\\nErzähle mir einen Witz.\\n\\nASSISTANT:\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a02f56e-09b4-4780-a056-f43efd2cb1d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "witz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618743b2-fe1f-4e57-b5ce-8be6626b929f",
   "metadata": {},
   "source": [
    "Hier sind weitere Tutorials, die zeigen, wie du Notebooks und LLMs nutzen kannst.\n",
    "\n",
    "- Tutorial zur [Text Generierung](https://jhub.mylab.th-luebeck.de/hub/user-redirect/git-pull?repo=https%3A%2F%2Fgit.mylab.th-luebeck.de%2Fgpu%2Ftutorials.git&urlpath=lab%2Ftree%2Ftutorials.git%2F31-textgeneration.ipynb&branch=main) (am Bsp. des HuggingFace Text-Generation Interfaces)\n",
    "- Tutorial zur [Nutzung von LangChain](https://jhub.mylab.th-luebeck.de/hub/user-redirect/git-pull?repo=https%3A%2F%2Fgit.mylab.th-luebeck.de%2Fgpu%2Ftutorials.git&urlpath=lab%2Ftree%2Ftutorials.git%2F32-langchain.ipynb&branch=main) (ausführlich)\n",
    "- Tutorial zu [Retrieval Augmented Generation](https://jhub.mylab.th-luebeck.de/hub/user-redirect/git-pull?repo=https%3A%2F%2Fgit.mylab.th-luebeck.de%2Fgpu%2Ftutorials.git&urlpath=lab%2Ftree%2Ftutorials.git%2F33-rag.ipynb&branch=main)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f736c18a-ec2a-4df4-a221-9e5c13ee16c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
