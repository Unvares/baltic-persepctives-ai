{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32754595-20c1-4826-a5f2-a4560ae86ff4",
   "metadata": {},
   "source": [
    "# Examples from the Retrieval Augmented Generation Video (Extended Version using the LangChain Expression Language, LCEL)\n",
    "\n",
    "**Author:** **Keno Teppris** (ask for support, even if you are not in his team, chair of Team #7, Baltic Perspectives, Room: 25-2.16)\n",
    "\n",
    "Here you will find all code parts from the Retrieval Augmented Generation video in the order in which they occur in the video.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Info:**\n",
    "The Streamlit example is not included in this notebook because it is not executable in Jupyter notebooks. You can find this example in the `../chatbot-rag` directory.\n",
    "</div>\n",
    "\n",
    "## Install necessary libraries\n",
    "\n",
    "To run the examples, please install the following libraries first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b75df7-8b6c-4caf-815a-cd81b1426057",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tiktoken langchain langchain-community langchain-text-splitters langchain-chroma langchain-huggingface langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d99508-e0c2-45f6-85e4-5d94647eba6a",
   "metadata": {},
   "source": [
    "# Introduction to RAG Indexing\n",
    "\n",
    "Welcome to the RAG (Retrieval-Augmented Generation) Indexing section! In this section, we will explore the innovative approach of integrating retrieval mechanisms with generative models to enhance the capabilities of AI systems. The RAG methodology empowers models to dynamically retrieve relevant information from a knowledge base (or \"document store\") during the generation process.\n",
    "\n",
    "<img src=\"https://js.langchain.com/v0.2/assets/images/rag_indexing-8160f90a90a33253d0154659cf7d453f.png\" width=\"800\"/>\n",
    "\n",
    "## Workflow Overview\n",
    "\n",
    "1. **Load:** The first step in the RAG process involves loading the necessary data. Data can be in various formats such as JSON files or URLs pointing to data sources. This is crucial for building a diverse and comprehensive document store.\n",
    "\n",
    "2. **Split:** Once data is loaded, it's split into manageable parts or chunks. Splitting is essential for both processing efficiency and effective data management.\n",
    "\n",
    "3. **Embed:** Each chunk of data is then embedded into a vector space. These embeddings represent the semantic content of the data in a format that machines can understand and compare.\n",
    "\n",
    "4. **Store:** Finally, the embeddings are stored in a structured format. This \"store\" acts as the retrieval database during the generation phase, allowing the model to access and utilize the information efficiently.\n",
    "\n",
    "5.  **Test:** Test the retriever by querying documents.\n",
    "\n",
    "By the end of this tutorial, you'll gain hands-on experience with each of these steps, understand how they interconnect, and appreciate their importance in building powerful AI models that can leverage external knowledge bases effectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257316b4-6a4d-4716-97bd-b0f425d30253",
   "metadata": {},
   "source": [
    "## 1. Load: Download your knowledge base\n",
    "\n",
    "Our intention is to develop an expert chatbot about prompt engineering. For this we use the contents of the page [promptingguide.ai](https://www.promptingguide.ai).\n",
    "With a little research, we find out that this page is generated from the following repository on [Github](https://github.com/dair-ai/Prompt-Engineering-Guide).\n",
    "And that's great, because we can then download the content of the repository directly and use it for our knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37732ede-9aa5-445a-a4d2-cc9972be8ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "url = 'https://github.com/dair-ai/Prompt-Engineering-Guide/archive/refs/heads/main.zip'\n",
    "response = requests.get(url)\n",
    "\n",
    "with zipfile.ZipFile(io.BytesIO(response.content)) as the_zip_file:\n",
    "    the_zip_file.extractall('./') \n",
    "print(\"File unzipped successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694ecf29-2ed9-4f21-a9cf-aa823b4f594b",
   "metadata": {},
   "source": [
    "## 2. Split: Import the relevant parts and do a little preprocessing\n",
    "\n",
    "If we look at the [Github repository](https://github.com/dair-ai/Prompt-Engineering-Guide), we see that the relevant and English language parts are in the [ar-pages](https://github.com/dair-ai/Prompt-Engineering-Guide/tree/main/ar-pages) directory and end with `.mdx` extensions. This will be the starting point of our knowledge base.\n",
    "\n",
    "MDX is a format that allows you to write JSX (JavaScript XML) embedded within Markdown content. This enables you to use React components directly in your Markdown files. MDX is commonly used in documentation sites and other React-based web applications to combine the simplicity of Markdown with the power of React components.\n",
    "\n",
    "And that's great again. Because language models are very good at understanding and generating Markdown. We are not interested in the JavaScript parts, but most of the file content is formatted in Markdown. So that should work.\n",
    "\n",
    "The following code splits the content of multiple `.ar.mdx` files into chunks and counts the number of chunks. It uses `tqdm` for a progress bar, `glob` to find files, and `RecursiveCharacterTextSplitter` to split text. This script iterates through all `.ar.mdx` files, reads their content, splits it into chunks, and appends each chunk to a list. Finally, it counts and returns the total number of chunks.\n",
    "\n",
    "To ensure that our texts fit into the context window of our embeddings (i.e. do not become too large) we use a `RecursiveCharacterTextSplitter`.  The `RecursiveCharacterTextSplitter` is a tool for dividing large texts into smaller chunks, typically for easier processing or analysis. It splits text into segments based on a specified maximum size, like 10,000 characters. The splitter ensures that each chunk is contextually meaningful by adjusting split points, avoiding breaks in the middle of words or sentences. This recursive approach helps manage large documents efficiently while maintaining readability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa43998f-3743-4a85-b403-9138e3495ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from glob import glob\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000)\n",
    "\n",
    "chunks = []\n",
    "\n",
    "for doc in tqdm(glob(\"Prompt-Engineering-Guide-main/ar-pages/**/*.ar.mdx\")):\n",
    "    with open(doc) as f:\n",
    "        for chunk in text_splitter.split_text(f.read()):\n",
    "            try:\n",
    "                chunks.append(chunk)\n",
    "            except Exception as ex:\n",
    "                print(doc, len(chunk), \"not processable\", str(ex))\n",
    "\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f3194a-e825-4606-af38-992baefe48c7",
   "metadata": {},
   "source": [
    "So we have about 80 articles about prompt engineering in our Prompt Engineering Guide, which we have broken down into just over 100 content chunks for our knowledge base.\n",
    "\n",
    "## 3.-4., Embed and store: Build your knowledge base\n",
    "\n",
    "We now only need to convert these into embedding vectors and save them in a vector store. We use Chroma as the vector store for this and work with an [BGE M3 Embedding](https://arxiv.org/abs/2402.03216). BGE M3 Embedding is characterised by its versatility in multi-linguality, multi-functionality and multi-granularity. It supports more than 100 working languages and is suitable for multilingual and cross-language retrieval tasks. It is capable of processing inputs of varying granularity, ranging from short sentences to long documents with up to 8192 tokens and demonstrates similar performance to the commercial OpenAI embeddings as the following comparison is showing.\n",
    "\n",
    "![width:250px](https://huggingface.co/BAAI/bge-m3/resolve/main/imgs/others.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a370395-28a3-4daa-832b-65bbbcc76ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface.embeddings import HuggingFaceEndpointEmbeddings\n",
    "\n",
    "bge_m3_embeddings = HuggingFaceEndpointEmbeddings(model=\"https://bge-m3-embedding.llm.mylab.th-luebeck.dev\")\n",
    "bge_m3 = Chroma.from_texts(chunks, bge_m3_embeddings, collection_name=\"bge_m3\")\n",
    "retriever = bge_m3.as_retriever(search_kwargs={'k': 3})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0296fd-cdfa-41df-af6c-0ac94263b496",
   "metadata": {},
   "source": [
    "## TASK: Test retriever by querying your knowledge base\n",
    "\n",
    "And now we come to the fun part. We ask our Knowledge Store for Retrieval Augmented Generation and get hits of indexed content chunks from the Prompt Engineering Guide that deal with this.\n",
    "\n",
    "To estimate how large the generated context will be that we will put into our language model, we use `tiktoken` and estimate the number of tokens that would be required for the GPT-3.5-turbo model (the assumption here is that this number of tokens should be about right for our Llama3 models as well).\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "**Transfer:**\n",
    "\n",
    "Try to adapt the code and provide an interactive query using `ipywidgets`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73706815-4f94-4119-8412-24bfa25687db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "tokens = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "\n",
    "docs = retriever.invoke(\"What is prompt chaining?\")\n",
    "for doc in docs:\n",
    "    print(\"---\")\n",
    "    print(len(doc.page_content))\n",
    "    print(doc.page_content)\n",
    "\n",
    "ctx = \"\\n\".join(d.page_content for d in docs)\n",
    "f\"{len(tokens.encode(ctx))} tokens\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfb1ba2-5e85-47ed-8eea-800f4e5fc254",
   "metadata": {},
   "source": [
    "OK, we see that for different examples, the token count is usually below the 5000 token limit of our Llama3 70B model (5000 tokens) (and actually always well below the 7500 input tokens of our Llama3 8B model). This should allow us to create an interactive prompt engineering guide.\n",
    "\n",
    "## TASK: Connect your Knowledge Base with your LLM using a Prompt Template\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "**Transfer:**\n",
    "\n",
    "Try to adapt the code and provide an interactive query and answer generation using `ipywidgets`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd704f7-ac59-47bd-acd3-1744c20639c2",
   "metadata": {},
   "source": [
    "# Example usage of LangChain Expression Language (LCEL)\n",
    "\n",
    "LangChain Expression Language (LCEL) is a declarative language for chaining LangChain components. It supports streaming, async execution, and optimized parallel execution. LCEL allows prototypes to be put into production without code changes, providing flexibility and reliability for complex chains.\n",
    "\n",
    "## Runnable Interface\n",
    "\n",
    "The Runnable interface standardizes how to define and invoke custom chains. It includes methods like `stream`, `invoke`, and `batch` for synchronous and asynchronous execution. Many LangChain components, such as chat models and output parsers, implement this interface, facilitating easy customization and integration. \n",
    "\n",
    "For more information, visit the [LangChain documentation](https://python.langchain.com/v0.2/docs/concepts/#langchain-expression-language)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd76be5-0e99-4a96-98bb-48aebfcaca4b",
   "metadata": {},
   "source": [
    "## Retrieval and Answering\n",
    "\n",
    "In this section, we will leverage our knowledge base to retrieve relevant document chunks. Using both the context and the query, we'll construct prompts that guide our model's response process. The model will then generate answers based on the provided context, demonstrating how dynamic retrieval enhances the quality and relevance of model outputs.\n",
    "\n",
    "<img src=\"https://python.langchain.com/v0.2/assets/images/rag_retrieval_generation-1046a4668d6bb08786ef73c56d4f228a.png\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb59f51-9fd2-440f-80db-63870a18b5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()\n",
    "llm = ChatOpenAI(\n",
    "    base_url=\"https://chat-large.llm.mylab.th-luebeck.dev/v1\",\n",
    "    api_key=\"-\",\n",
    "    streaming=True,\n",
    "    max_tokens=3000\n",
    ")\n",
    "\n",
    "chain = llm | parser\n",
    "output = chain.invoke(\"Hi, how are you?\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8506939f-7d33-4fbe-91b7-97c95307d97f",
   "metadata": {},
   "source": [
    "The code provided sets up a language model chain using LangChain. Here's what happens:\n",
    "\n",
    "1. **Parser Setup**: A `StrOutputParser` is initialized to process the output into a string format.\n",
    "2. **Language Model Configuration**: `ChatOpenAI` is instantiated with a base URL, an API key, streaming enabled, and a maximum token limit of 3000.\n",
    "3. **Chain Creation**: The language model and parser are combined into a chain using the `|` operator.\n",
    "4. **Invocation**: The chain is invoked with the input \"Hi, how are you?\", and the output is printed.\n",
    "\n",
    "This setup processes and returns a response from the language model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535561b6-2b45-45fc-8537-ab77007b95a5",
   "metadata": {},
   "source": [
    "## Streaming\n",
    "The following code sets up an asynchronous chain to process input using LangChain. Here's what happens:\n",
    "\n",
    "1. **Asynchronous Invocation**: Uses `astream` to asynchronously process the input \"Hi, how are you?\" and print each chunk of the response in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d25bed-1934-4168-8963-62588eaacfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "async for chunk in chain.astream(\"Hi, how are you?\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a08be7-28a7-43ff-830e-8a9671d54537",
   "metadata": {},
   "source": [
    "### Introduction to Prompts\n",
    "\n",
    "This code demonstrates how to create and use a prompt with LangChain for generating responses:\n",
    "\n",
    "1. **Prompt Template**: A `ChatPromptTemplate` is created with a template string, \"tell me a joke about {topic}\".\n",
    "2. **Chain Creation**: The prompt template is combined with a language model (`llm`) and a parser (`parser`) to form a chain.\n",
    "3. **Asynchronous Invocation**: The chain is invoked asynchronously with the topic \"Large Language Models\", and each chunk of the response is printed in real-time.\n",
    "\n",
    "This setup dynamically generates and processes prompts for flexible interactions with the language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4d0226-85e0-465b-add6-268cee0f2284",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "async for chunk in chain.astream({\"topic\": \"Large Language Models\"}):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbc51ee-c1ce-482b-8725-f7c7b902106c",
   "metadata": {},
   "source": [
    "# Introduction to Question-Answering Chains using LangChain\n",
    "\n",
    "In this section, we will explore how to create a question-answering chain using LangChain, a powerful tool for integrating language models with retrieval-based systems. We build a system that can fetch relevant context from a set of documents and use this context to provide concise and accurate answers to questions.\n",
    "\n",
    "The key component of our system is:\n",
    "1. **Question-Answering Chain**: This uses the retrieved context to generate an answer to the question.\n",
    "\n",
    "We will start by incorporating the retriever into a question-answering chain using a specific prompt format. This prompt instructs the system on how to use the retrieved context to answer the question. The prompt emphasizes providing concise answers within three sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47209b0e-63ad-42a9-b0b1-3a0731fa9c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# 2. Incorporate the retriever into a question-answering chain.\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\" # the context variable will be filled with the retrieved context and is expected from any predefined rag chain\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e4c646-3f8b-4a5a-a89c-e3ee93e03cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49259ba-b4b1-4a5d-b2ae-b7f38ecc49c3",
   "metadata": {},
   "source": [
    "In this code:\n",
    "- `system_prompt` defines how the assistant should respond using the retrieved context.\n",
    "- `prompt` structures the interaction between the system and the user.\n",
    "- `question_answer_chain` combines the language model with the prompt to create answers. The word \"stuff\" refers to just combine all the retrieved context chunks. For other chain types, checkout the langchain documentation.\n",
    "- `rag_chain` ties everything together, allowing the system to retrieve relevant documents and use them to answer questions.\n",
    "\n",
    "This setup enables us to build an efficient question-answering system that leverages the strengths of both retrieval-based and generative models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fce42b-4747-465e-938e-28b182d7b7bf",
   "metadata": {},
   "source": [
    "## Question-Answering Chain to Retrieve and Answer Questions\n",
    "\n",
    "In this section, we will see how to use the question-answering chain we created to answer a specific question. Let's take a look at the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57517ac-21af-46f1-8900-f79d6b2d61e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What are context transformer and why should i use them?\"\n",
    "\n",
    "result = rag_chain.invoke({\"input\": question})\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf70ec06-7d99-478b-b835-ef6b75d39d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.keys())  # Output: dict_keys(['input', 'context', 'answer'])\n",
    "\n",
    "print(\"Question:\", result[\"input\"])\n",
    "print(\"-\"*10)\n",
    "print(\"First context:\", result[\"context\"][0].page_content[:500] + \"\\n[...]\")\n",
    "print(\"-\"*10)\n",
    "print(\"Answer:\", result[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2617266-702d-4ac3-8411-1ca4431930ed",
   "metadata": {},
   "source": [
    "In this code:\n",
    "- We define a question that we want the system to answer.\n",
    "- We use the `rag_chain` we created earlier to process the question. The `invoke` method of `rag_chain` is called with the question as input.\n",
    "- The result is stored in the `result` variable, and we print the answer using `result[\"answer\"]`.\n",
    "\n",
    "When we call `result.keys()`, we get `dict_keys(['input', 'context', 'answer'])`. This means that the result is a dictionary containing three keys:\n",
    "1. **input**: This is the original question we provided to the system.\n",
    "2. **context**: This contains the pieces of retrieved context that the system used to generate the answer.\n",
    "3. **answer**: This is the concise answer generated by the system based on the provided context.\n",
    "\n",
    "### Why does the result dictionary contain these keys?\n",
    "\n",
    "- **input**: Keeping the original input question helps in debugging and understanding what question was asked, especially when dealing with multiple questions.\n",
    "- **context**: This key provides transparency and traceability, allowing us to see the exact pieces of information the system used to derive its answer. This is crucial for understanding the reasoning process of the model and ensuring that the context used is relevant and accurate.\n",
    "- **answer**: The final answer provided by the system, which is what we are primarily interested in.\n",
    "\n",
    "In your application you can use this information to provide metadata or links to the source files, references in your applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade81c82-4686-4dab-9d54-735ff2f72508",
   "metadata": {},
   "source": [
    "## Creating a History-Aware Question-Answering Chain\n",
    "\n",
    "In this lesson, we will extend our question-answering system to be aware of the conversation history. This enhancement allows the system to handle questions that reference previous interactions, providing more contextually accurate answers.\n",
    "\n",
    "We'll go through the steps to create a history-aware retriever and integrate it with our existing retrieval chain.\n",
    "\n",
    "### Key Components\n",
    "\n",
    "1. **History-Aware Retriever**: Reformulates questions considering the conversation history.\n",
    "2. **Conversational Chain**: Maintains the conversation history across multiple interactions.\n",
    "\n",
    "Here is the code to set up these components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cbacc1-3801-4dcf-bb16-b6c04681ffaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e18b1c-17d1-4a2c-87c5-5508172b99cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We reuse our llm and retriever\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")\n",
    "\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfccea4-b4aa-41d2-b0d9-519e3f069b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store = {}\n",
    "\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4033685c-f493-40ab-8e72-e53f4b54f179",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "\n",
    "1. **contextualize_q_system_prompt**: This prompt ensures that the system reformulates the user's question to be self-contained, removing dependencies on the previous chat history.\n",
    "   \n",
    "2. **contextualize_q_prompt**: This is a `ChatPromptTemplate` that uses the system prompt and includes placeholders for the chat history and the user's input.\n",
    "\n",
    "3. **history_aware_retriever**: This retriever uses the `contextualize_q_prompt` to reformulate questions considering the chat history.\n",
    "\n",
    "4. **rag_chain**: We recreate our retrieval chain using the history-aware retriever.\n",
    "\n",
    "5. **ChatMessageHistory**: This class manages the chat history for different sessions.\n",
    "\n",
    "6. **get_session_history**: This function retrieves the chat history for a given session ID, creating a new history if one doesn't exist.\n",
    "\n",
    "7. **RunnableWithMessageHistory**: This class integrates the `rag_chain` with the session-based message history, allowing the system to maintain context across interactions.\n",
    "\n",
    "### How It Works\n",
    "\n",
    "- The system now keeps track of the conversation history for each session.\n",
    "- When a new question is asked, the `history_aware_retriever` reformulates it using the chat history to make it self-contained.\n",
    "- The reformulated question is then processed by the retrieval chain to provide a contextually accurate answer.\n",
    "\n",
    "This setup enhances the system's ability to handle follow-up questions and references to previous interactions, making it more effective in conversational settings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22ae0e2-4d58-4602-9a2a-3ae8e67b517c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = conversational_rag_chain.invoke(\n",
    "    {\"input\": \"How to add citations to a RAG chain?\"},\n",
    "    config={\n",
    "        \"configurable\": {\"session_id\": \"abc12\"}\n",
    "    },  # constructs a key \"abc123\" in `store`.\n",
    ")\n",
    "\n",
    "print(output[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d5c373-8ee8-4c47-a84c-68bb6a41e942",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = conversational_rag_chain.invoke(\n",
    "    {\"input\": \"How could a citation prompt look like?\"},\n",
    "    config={\n",
    "        \"configurable\": {\"session_id\": \"abc12\"}\n",
    "    }\n",
    ")\n",
    "\n",
    "print(output[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0edfe9a-3007-4637-a81d-a01368899a25",
   "metadata": {},
   "source": [
    "### Access message history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbae26a4-9605-49e6-bf3a-70b5914e17f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "markdown_string = \"\"\n",
    "\n",
    "for message in store[\"abc12\"].messages:\n",
    "    prefix = \"AI\" if isinstance(message, AIMessage) else \"User\"\n",
    "\n",
    "    markdown_string += f\"\\n**{prefix}:** {message.content}\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0480d1ff-3d98-4de4-984d-5fab0075efd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(Markdown(markdown_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7172d24-1a7a-40ee-9808-38047599b8f5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## TASK Advanced: Add citations to the answer.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "**Transfer:**\n",
    "\n",
    "Checkout the langchain tutorial to add citations to your answer output: [QA Citations](https://python.langchain.com/v0.2/docs/how_to/qa_citations/)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedab435-644a-4f4a-94c7-8a1c0466bb5c",
   "metadata": {},
   "source": [
    "# Enhancing the Question-Answering Chain with Contextual Compression\n",
    "\n",
    "Congratulations on building a history-aware question-answering system! By now, you've seen how integrating chat history into the retrieval and question-answering process can significantly improve the system's ability to provide contextually accurate answers. But the journey doesn't have to end here. There are always ways to refine and enhance your system further.\n",
    "\n",
    "## Next Steps: Contextual Compression\n",
    "\n",
    "One powerful technique you can explore to improve your question-answering chain is **contextual compression**. Contextual compression involves reducing the amount of context while retaining the most relevant information. This can help in scenarios where the retrieved context is too large to process efficiently or when you want to focus on the most critical parts of the context.\n",
    "\n",
    "To implement contextual compression, you can follow the guide provided in the LangChain documentation:\n",
    "\n",
    "### Steps to Implement Contextual Compression\n",
    "\n",
    "1. **Understand Contextual Compression**: Read through the [LangChain documentation on contextual compression](https://python.langchain.com/v0.2/docs/how_to/contextual_compression/) to get a detailed understanding of the concept and its benefits.\n",
    "\n",
    "2. **Integrate Compression Techniques**: Use the techniques described in the documentation to compress the context retrieved by your system. This could involve summarizing documents, extracting key phrases, or using machine learning models to identify the most relevant pieces of information.\n",
    "\n",
    "3. **Update Your Chain**: Modify your existing retrieval and question-answering chain to include a step for contextual compression. This could be done by creating a new component in the chain that processes the retrieved context before it's used to generate the answer.\n",
    "\n",
    "4. **Experiment and Evaluate**: Test your improved system with various questions and chat histories. Evaluate the quality of the answers and the efficiency of the system. Compare it with the previous version to see the improvements.\n",
    "\n",
    "\n",
    "By incorporating contextual compression, you can make your question-answering system even more robust and efficient, providing high-quality answers while handling larger and more complex contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae40072-e439-41c1-871a-bfde4f177701",
   "metadata": {},
   "source": [
    "## TASK Advanced RAG: Add Pre or Post Retrieval components \n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "**Transfer:**\n",
    "\n",
    "Try out different predefined RAG components and add them to your chain to see if it improves the results: [Contextual Compression](https://python.langchain.com/v0.2/docs/how_to/contextual_compression/)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4a9fa2-7928-4f95-8104-2a03d239e970",
   "metadata": {},
   "source": [
    "Great. We hope this notebook has helped you to understand how the answer generation of large language models can be guided using trusted knowledge stores. This should reduce hallucination effects.\n",
    "\n",
    "If you have any questions, please do not hesitate to ask Keno (Track chair of Baltic Perspectives), even if you are not a member of his team. He is in room 25-2.16.\n",
    "\n",
    "<img src=\"https://mylab.th-luebeck.de/images/mylab-logo-without.png\" width=200px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a6b8d9-fba4-4f05-9aeb-a6cd20f3f53f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
