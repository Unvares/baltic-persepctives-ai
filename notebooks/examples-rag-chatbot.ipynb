{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32754595-20c1-4826-a5f2-a4560ae86ff4",
   "metadata": {},
   "source": [
    "# Examples from the Retrieval Augmented Generation Video\n",
    "\n",
    "Here you will find all code parts from the Retrieval Augmented Generation video in the order in which they occur in the video.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Info:**\n",
    "The Streamlit example is not included in this notebook because it is not executable in Jupyter notebooks. You can find this example in the `../chatbot-rag` directory.\n",
    "</div>\n",
    "\n",
    "## Install necessary libraries\n",
    "\n",
    "To run the examples, please install the following libraries first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b75df7-8b6c-4caf-815a-cd81b1426057",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tiktoken langchain-text-splitters langchain-chroma langchain-huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257316b4-6a4d-4716-97bd-b0f425d30253",
   "metadata": {},
   "source": [
    "## Part 1: Download your knowledge base\n",
    "\n",
    "Our intention is to develop an expert chatbot about prompt engineering. For this we use the contents of the page [promptingguide.ai](https://www.promptingguide.ai).\n",
    "With a little research, we find out that this page is generated from the following repository on [Github](https://github.com/dair-ai/Prompt-Engineering-Guide).\n",
    "And that's great, because we can then download the content of the repository directly and use it for our knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37732ede-9aa5-445a-a4d2-cc9972be8ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "url = 'https://github.com/dair-ai/Prompt-Engineering-Guide/archive/refs/heads/main.zip'\n",
    "response = requests.get(url)\n",
    "\n",
    "with zipfile.ZipFile(io.BytesIO(response.content)) as the_zip_file:\n",
    "    the_zip_file.extractall('./') \n",
    "print(\"File unzipped successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694ecf29-2ed9-4f21-a9cf-aa823b4f594b",
   "metadata": {},
   "source": [
    "## Part 2: Import the relevant parts and do a little preprocessing\n",
    "\n",
    "If we look at the [Github repository](https://github.com/dair-ai/Prompt-Engineering-Guide), we see that the relevant and English language parts are in the [ar-pages](https://github.com/dair-ai/Prompt-Engineering-Guide/tree/main/ar-pages) directory and end with `.mdx` extensions. This will be the starting point of our knowledge base.\n",
    "\n",
    "MDX is a format that allows you to write JSX (JavaScript XML) embedded within Markdown content. This enables you to use React components directly in your Markdown files. MDX is commonly used in documentation sites and other React-based web applications to combine the simplicity of Markdown with the power of React components.\n",
    "\n",
    "And that's great again. Because language models are very good at understanding and generating Markdown. We are not interested in the JavaScript parts, but most of the file content is formatted in Markdown. So that should work.\n",
    "\n",
    "The following code splits the content of multiple `.ar.mdx` files into chunks and counts the number of chunks. It uses `tqdm` for a progress bar, `glob` to find files, and `RecursiveCharacterTextSplitter` to split text. This script iterates through all `.ar.mdx` files, reads their content, splits it into chunks, and appends each chunk to a list. Finally, it counts and returns the total number of chunks.\n",
    "\n",
    "To ensure that our texts fit into the context window of our embeddings (i.e. do not become too large) we use a `RecursiveCharacterTextSplitter`.  The `RecursiveCharacterTextSplitter` is a tool for dividing large texts into smaller chunks, typically for easier processing or analysis. It splits text into segments based on a specified maximum size, like 10,000 characters. The splitter ensures that each chunk is contextually meaningful by adjusting split points, avoiding breaks in the middle of words or sentences. This recursive approach helps manage large documents efficiently while maintaining readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa43998f-3743-4a85-b403-9138e3495ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from glob import glob\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000)\n",
    "\n",
    "chunks = []\n",
    "\n",
    "for doc in tqdm(glob(\"Prompt-Engineering-Guide-main/ar-pages/**/*.ar.mdx\")):\n",
    "    with open(doc) as f:\n",
    "        for chunk in text_splitter.split_text(f.read()):\n",
    "            try:\n",
    "                chunks.append(chunk)\n",
    "            except Exception as ex:\n",
    "                print(doc, len(chunk), \"not processable\", str(ex))\n",
    "\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f3194a-e825-4606-af38-992baefe48c7",
   "metadata": {},
   "source": [
    "So we have about 80 articles about prompt engineering in our Prompt Engineering Guide, which we have broken down into just over 100 content chunks for our knowledge base.\n",
    "\n",
    "## Part 3: Build your knowledge base\n",
    "\n",
    "We now only need to convert these into embedding vectors and save them in a vector store. We use Chroma as the vector store for this and work with an [BGE M3 Embedding](https://arxiv.org/abs/2402.03216). BGE M3 Embedding is characterised by its versatility in multi-linguality, multi-functionality and multi-granularity. It supports more than 100 working languages and is suitable for multilingual and cross-language retrieval tasks. It is capable of processing inputs of varying granularity, ranging from short sentences to long documents with up to 8192 tokens and demonstrates similar performance to the commercial OpenAI embeddings as the following comparison is showing.\n",
    "\n",
    "![width:250px](https://huggingface.co/BAAI/bge-m3/resolve/main/imgs/others.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a370395-28a3-4daa-832b-65bbbcc76ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface.embeddings import HuggingFaceEndpointEmbeddings\n",
    "\n",
    "bge_m3_embeddings = HuggingFaceEndpointEmbeddings(model=\"https://bge-m3-embedding.llm.mylab.th-luebeck.dev\")\n",
    "bge_m3 = Chroma.from_texts(chunks, bge_m3_embeddings, collection_name=\"bge_m3\")\n",
    "knowledge_base = bge_m3.as_retriever(search_kwargs={'k': 3})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0296fd-cdfa-41df-af6c-0ac94263b496",
   "metadata": {},
   "source": [
    "## Part 4: Query your knowledge base\n",
    "\n",
    "And now we come to the fun part. We ask our Knowledge Store for Retrieval Augmented Generation and get hits of indexed content chunks from the Prompt Engineering Guide that deal with this.\n",
    "\n",
    "To estimate how large the generated context will be that we will put into our language model, we use `tiktoken` and estimate the number of tokens that would be required for the GPT-3.5-turbo model (the assumption here is that this number of tokens should be about right for our Llama3 models as well).\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "**Transfer:**\n",
    "\n",
    "Try to adapt the code and provide an interactive query using `ipywidgets`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73706815-4f94-4119-8412-24bfa25687db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "tokens = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "\n",
    "docs = knowledge_base.invoke(\"What is prompt chaining?\")\n",
    "for doc in docs:\n",
    "    print(\"---\")\n",
    "    print(len(doc.page_content))\n",
    "    print(doc.page_content)\n",
    "\n",
    "ctx = \"\\n\".join(d.page_content for d in docs)\n",
    "f\"{len(tokens.encode(ctx))} tokens\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfb1ba2-5e85-47ed-8eea-800f4e5fc254",
   "metadata": {},
   "source": [
    "OK, wir sehen das f√ºr unterschiedliche Beispiele, die Tokenanzahl meist unter der 5000 Tokengrenze unseres Llama3 70B Modells (5000 Token) liegt (und eigentlich immer deutlich unter den 7500 Input Tokens unseres Llama3 8B Modells). Damit sollten wir also einen interaktiven Prompt Engineering Guide hinbekommen.\n",
    "\n",
    "## Part 5: Connect your Knowledge Base with your LLM using a Prompt Template\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "**Transfer:**\n",
    "\n",
    "Try to adapt the code and provide an interactive query and answer generation using `ipywidgets`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be080b28-730a-4108-8f93-a146b3c06bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "query = \"Show me examples of RAG.\"\n",
    "\n",
    "client = OpenAI(base_url=\"https://chat-large.llm.mylab.th-luebeck.dev/v1\", api_key=\"-\")\n",
    "\n",
    "docs = knowledge_base.invoke(query)\n",
    "context = \"\\n\".join(d.page_content for d in docs)\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are KIRA, a prompt engineering expert. You answer questions based on the context you have retrieved from your knowledge base.\"},\n",
    "        {\"role\": \"system\", \"content\": f\"Context: {context}\"},\n",
    "        {\"role\": \"user\", \"content\": query }\n",
    "    ],\n",
    "    model=\"\", stream=True, max_tokens=3000\n",
    ")\n",
    "\n",
    "print(f\"Generated output based on {len(tokens.encode(context))} tokens:\")\n",
    "for message in chat_completion:\n",
    "    if not message.choices[0].finish_reason:\n",
    "        print(message.choices[0].delta.content, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4a9fa2-7928-4f95-8104-2a03d239e970",
   "metadata": {},
   "source": [
    "Great. We hope this notebook has helped you to understand how the answer generation of large language models can be guided using trusted knowledge stores. This should reduce hallucination effects.\n",
    "\n",
    "If you have any questions, please do not hesitate to ask them. Our staff will see what we can do.\n",
    "\n",
    "<img src=\"https://mylab.th-luebeck.de/images/mylab-logo-without.png\" width=200px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293a5bdb-93d6-4c0a-a95f-6990630e5e35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
